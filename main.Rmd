---
title: "Práctica 9 PLN"
output: pdf_document
author: "Pablo de Tarso Pedraz, Jorge López, Martín Hernández y Pablo Suárez"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Parte 1: Creación del corpus

Se va a hacer scraping de la siguiente página: : https://www.conceptosjuridicos.com/codigo-penal/, la cual contiene artículos del código penal. Primero importamos las funciones creadas en los demás archivos 

```{r}
source("./data/generador.R")
source("./data/processor.R")
source("./install_requirements.R")

#writeAll() solo si necesita descargar de nuevo los archivos

```

Instalamos las dependecias
```{r results='hide'}
start_install()
library(quanteda)

```

Con la siguiente función conseguimos el contenido de los artículos (Aun no tienen las cabeceras)

```{r pressure}
articulos = unlist(get_content())
substr(head(articulos),1,80)

```

Despues usamos nuestro separador, que devuelve los nombres, docvars y cabeceras

```{r}


utils = separator()
corpus_names = utils$names
corpus_starts = utils$starts
corpus_docvars = utils$docvars
head(corpus_names)
head(corpus_starts)
head(corpus_docvars)
```
Nótese que en los docvars habrá NA's pues no todos los artículos tiene Libro/Título/Capítulo/Sección. Por ejemplo los primeros no están incluídos dentro de ningún libro.
Después unimos las cabeceras con el resto del contenido de los artículos. 

```{r}
for ( i in 1:length(articulos)){
  articulos[[i]] = paste(corpus_starts[[i]],articulos[[i]],sep = "\n")
}

```

Y ya tenemos todo

```{r}
corpus = corpus(articulos)
docvars(corpus) = corpus_docvars
names(corpus) = corpus_names
head(corpus)
cat(corpus[[1]])
```

## Parte 2: Distancias 

Primero creamos la matriz de distancias entre los textos.

```{r}
dtm = dfm(tokens(corpus))
m <- as.matrix(dtm)
distMatrix <- dist(m, method = "euclidean")
```

Después agrupamos los textos usando las distancias para hacer el dendrograma

```{r}

groups <- hclust(distMatrix)

plot(groups, cex = 0.9, hang = -1, 
     labels=FALSE,
     main = "Dendrograma Código Penal",
     xlab = "Artículos",
) #labels = FALSE para que no aparezcan los nombres, sino no se aprecia nada 

```
Por la cantidad de datos en el corpus es casi imposible apreciar nada, mucho menos sin tener los nombres. Sin embargo, si mostramos los nombres la pantalla se tapa con ellos al completo.

Para buscar los dos artículos más parecidos buscamos la menor distancia. Primero tenemos que transformar distMatrix en una matriz, pues de normal se devuelve en forma de tipo "dist", que contiene, entre otras cosas, el vector de distancias. Sin embargo, es más cómodo en forma de matriz. Despues cambiamos la diagonal de esta matriz a infinito, pues la diagonal representa la distancia de cada texto a sí mismo y este será 0 siempre.

```{r}


distMatrix_full <- as.matrix(distMatrix)

diag(distMatrix_full) <- Inf

min_index <- which(distMatrix_full == min(distMatrix_full), arr.ind = TRUE)
i <- min_index[1, 1]
j <- min_index[1, 2]
cat(i,j)
```

Así, los dos textos más parecidos son:

```{r, results='markup'}
text_1 <- strwrap(corpus[[i]], width = 80)
text_2 <- strwrap(corpus[[2]], width = 80)
cat(paste(text_1, collapse = "\n"))
cat(paste(text_2, collapse = "\n"))

```
Nótese que se ha hecho un wrap pues sino los textos se salen del pdf, por lo que no se estan consevando los saltos de línea.


## Parte 3: Entidades nombradas

Importamos primero las librerias necesarias y cargamos los modelos 

```{r}
library(spacyr)
library(udpipe)

spacy_initialize(model = "es_core_news_sm")
#ud_model <- udpipe_download_model(language = "spanish-ancora")
udmodel_es <- udpipe_load_model(file = 'spanish-ancora-ud-2.5-191206.udpipe')


```

Buscamos la entidades nombradas con scapyr:

```{r}

ent = spacy_extract_entity(corpus)
head(ent)
```
Ahora veamos las más comunes

```{r}
freqs = table(ent$text)
comunes = freqs[order(-freqs)][1:20]
comunes

```
Podemos ver que las palabras que han salido casi todas pertenecen a las cabeceras de los artículos, como era de esperar. Además de otras palabras comunes en textos legales, como delito, código...

Ahora hagámoslo con Udpipe:
Primero anotamos 

```{r}
annotations <- udpipe_annotate(udmodel_es, x = corpus)
annotations_df <- as.data.frame(annotations)
head(annotations_df[,5:10])
```

Y usamos keywords_rake
```{r}
keywords <- keywords_rake(
  annotations_df,    
  term = "lemma",        #Usamos la columna "lemma" de la anotaciones
  group = "doc_id"       #Agrupaciones segun doc_id
)
head(keywords)
```

Parece que no esta detectando nada. Esto puede ocurrir por frecuencia de elementos como puntuación

```{r}
keywords <- keywords_rake(
  annotations_df,    
  term = "lemma",        #Usamos la columna "lemma" de la anotaciones
  group = "doc_id",       #Agrupaciones segun doc_id
  relevant = (annotations_df$xpos != "PUNCT")  #Nos permite seleccionar cuales filas son relevantes 
  )

keywords = keywords[order(-keywords$freq),]   #Ordenamos nuestro dataframe en 
#orden descendente según la frecuencia 
head(keywords)
```

Vemos que muchas letras sueltas y números que no nos interesan demasiado. Veamos como arreglar esto 

```{r}
table(annotations_df$xpos)
```

Algunos de estos no nos interesan: Por ejemplo:
```{r}
head(annotations_df[which(annotations_df$upos=="SYM"),6])
head(annotations_df[which(annotations_df$upos=="NUM"),6])
```

Entonces los quitaremos con el parametro relevant de keywords_rake (permite decir que filas no son relevantes). También podemos modificar ngram_max, que cambia la máxima cantidad de palabras que puede tener en cuenta como una entidad (por defecto es solo 2)

```{r}
keywords <- keywords_rake(
  annotations_df,    
  term = "lemma",        
  group = "doc_id",
  relevant = ( ! annotations_df$xpos  %in% c("PUNCT","SYM","NUM")),
  ngram_max = 5,
)

keywords = keywords[order(-keywords$freq),]  #Ordenamos for frecuencia

head(keywords,20)
```

Siguen saliendo muchas palabras que no nos interesan "a", "y", "el que". Para ello quitaremos otras categorías como las conjunciones y los determinantes. Así se concentrará más en los sustantivos y adjetivos

```{r}
keywords <- keywords_rake(
  annotations_df,    
  term = "lemma",        
  group = "doc_id",
  relevant = ( ! annotations_df$xpos  %in% c("PUNCT","SYM","NUM","CCONJ","ADP")),
  ngram_max = 5,
)

keywords = keywords[order(-keywords$freq),]  #Ordenamos for frecuencia

head(keywords,20)
```
Ahora parece representar algo mejor los textos.
Podemos ver que coincide en algunas con las calculadas con spacyr, "delitos", "año", "juez".


## Otros:

Se han creado varias funciones auxiliares para dividir en bloques la tarea de hacer scraping y formatear los datos, de esta manera es más fácil de buscar errores y organizar el código. El resto de los archivos son:

- **Schema.txt**: contiene un esquema de las cabeceras de los artículos. Esto fue obtenido de antemano usando XPATH

- **generador.R**:
    - get_html_arts: devuelve los links de los artículos que nos interesan
    - writeAll: escribe en la carpeta artículos el contenido (en html) de los artículos
    
- **processor.R**:
    - get_articles: devuelve los nombres de los artículos que nos interesan en el formato deseado (Articulo 10)
    - get_content: recibe el contenido html de un artículo y devuelve el contenido real del artículo del código penal
    - get_titles: itera sobre schema.txt y crea un dataframe que incluye cada artículo con sus cabeceras (Libro, Título, Capítulo, Sección)
    - separator: utiliza el dataframe dado por get_titles y lo separa en 3 elementos: 1. un dataframe `<<docvars>>` que contiene el mismo dataframe anterior pero quitando el inicio de las cabeceras, es decir, pasa de `<<LIBRO I: de las disposiciones...>>` a `<<de las disposiciones...>>`, 2. un vector `<<names>>`, que contiene el nombre de cada artículo con el fomato `<<LIBRO I.Título II.Capítulo I.Artículo III.>>` (Por ejemplo) y 3. un vector `<<starts>>` que contiene los inicios de los artículos, que es solamente la concatenación, con  `"\n"` como separador, de las cabeceras al completo de cada artículo
    
En un principio el código se ejecuta en local, pues processor.R lee de la carpeta artículos, las funciones del archivo generador no estan activas en el Rmd. Puede reescribir los artículos descomentando la primera función al principio #writeAll(). Lo mismo con el modelo de udpipe, de da ya descargado per se puede reinstalar descomentando la línea 134.
