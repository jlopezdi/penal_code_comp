---
title: "Práctica PLN: Grupo 9"
output: pdf_document
author: "Pablo de Tarso Pedraz, Jorge López, Martín Hernández y Pablo Suárez"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


Descargamos las funciones auxiliares creadas (documentadas al final)
```{r}
source("./data/generador.R")
source("./data/processor.R")
source("./install_requirements.R")
```

## Importante:

Todo el código funciona una vez ya se tienen los artículos guardados en la carpeta data, sin embargo no podemos enviar el zip con todos los artículos ya descargados debido al peso, para ello dejamos aqui la función que los genera. Solo se activará si falta alguno de los artículos. Nótese que writeAll tarda mucho en generar todos los archivos, por tanto, la primera vez que se ejecute tardará un rato en terminar. Sin embargo, una vez todos esten descargados ya funciona el resto en local sin necesidad de cambiar este código, si falta unos pocos archivos, la propia función writeAll se encarga de solo leer y descargar esos por lo que tardará mucho menos.

```{r, warning = FALSE, message=FALSE,results='hide'}
articulos = get_articles()
downloaded = list.files("./data/articulos/")
for ( i in 1:length(articulos)){
  art = gsub(" ","_",articulos[i])
  file_name = paste(art,".txt",sep="")
  if (!(file_name %in%  downloaded)){
    writeAll()
    break
  }
}
```

## Parte 1: Creación del corpus

Se va a hacer scraping de la siguiente página: : https://www.conceptosjuridicos.com/codigo-penal/, la cual contiene artículos del código penal. Primero importamos las funciones creadas en los demás archivos 


Instalamos las dependecias
```{r results='hide'}
start_install()
library(quanteda)

```

Con la siguiente función conseguimos el contenido de los artículos (Aun no tienen las cabeceras)

```{r pressure}
articulos = unlist(get_content())
substr(head(articulos,5),1,80)

```

Despues usamos nuestro separador, que devuelve los nombres, docvars y cabeceras

```{r}
utils = separator()
corpus_names = utils$names
corpus_starts = utils$starts
corpus_docvars = utils$docvars
```

Corpus names:
```{r echo=FALSE}
head(corpus_names,5)
cat("...")
tail(corpus_names,5)
```

Corpus_starts:
```{r echo=FALSE}
head(corpus_starts,5)
cat("...")
tail(corpus_starts,5)
```

Corpus_docvars:
```{r echo=FALSE}
head(corpus_docvars,5)
cat("...")
tail(corpus_docvars,5)
```



Nótese que en los docvars habrá NA's pues no todos los artículos tiene Libro/Título/Capítulo/Sección. Por ejemplo los primeros no están incluídos dentro de ningún libro.
Después unimos las cabeceras con el resto del contenido de los artículos. 

```{r}
for ( i in 1:length(articulos)){
  articulos[[i]] = paste(corpus_starts[[i]],articulos[[i]],sep = "\n")
}

```

Y ya tenemos todo

```{r}
corpus = corpus(articulos)
docvars(corpus) = corpus_docvars
names(corpus) = corpus_names

```
```{r echo=FALSE}
head(corpus)
cat("...")
tail(corpus)
```
```{r}
cat(corpus[[1]])
```


## Parte 2: Distancias 

Primero creamos la matriz de distancias entre los textos.

```{r}
dtm = dfm(tokens(corpus))
m <- as.matrix(dtm)
distMatrix <- dist(m, method = "euclidean")
```

Después agrupamos los textos usando las distancias para hacer el dendrograma

```{r}

groups <- hclust(distMatrix)

plot(groups, cex = 0.9, hang = -1, 
     labels=FALSE,
     main = "Dendrograma Código Penal",
     xlab = "Artículos",
) #labels = FALSE para que no aparezcan los nombres, sino no se aprecia nada 

```
Por la cantidad de datos en el corpus es casi imposible apreciar nada, mucho menos sin tener los nombres. Sin embargo, si mostramos los nombres la pantalla se tapa con ellos al completo.

Para buscar los dos artículos más parecidos buscamos la menor distancia. Primero tenemos que transformar distMatrix en una matriz, pues de normal se devuelve en forma de tipo "dist", que contiene, entre otras cosas, el vector de distancias. Sin embargo, es más cómodo en forma de matriz. Despues cambiamos la diagonal de esta matriz a infinito, pues la diagonal representa la distancia de cada texto a sí mismo y este será 0 siempre.

```{r}


distMatrix_full <- as.matrix(distMatrix)

diag(distMatrix_full) <- Inf

min_index <- which(distMatrix_full == min(distMatrix_full), arr.ind = TRUE)
i <- min_index[1, 1]
j <- min_index[1, 2]
cat(i,j)
```

Así, los dos textos más parecidos son:

```{r, results='markup'}
text_1 <- strwrap(corpus[[i]], width = 80)
text_2 <- strwrap(corpus[[2]], width = 80)
cat(paste(text_1, collapse = "\n"))
cat(paste(text_2, collapse = "\n"))

```
Nótese que se ha hecho un wrap pues sino los textos se salen del pdf, por lo que no se estan consevando los saltos de línea.


## Parte 3: Entidades nombradas

Importamos primero las librerias necesarias y cargamos los modelos 

```{r}
library(spacyr)
library(udpipe)

spacy_initialize(model = "es_core_news_sm")
#ud_model <- udpipe_download_model(language = "spanish-ancora")
udmodel_es <- udpipe_load_model(file = 'spanish-ancora-ud-2.5-191206.udpipe')


```

Buscamos la entidades nombradas con scapyr:

```{r}

ent = spacy_extract_entity(corpus)

```
```{r echo=FALSE}
head(ent)
cat("...")
tail(ent)
```
Ahora veamos las más comunes

```{r}
freqs = table(ent$text)
comunes = freqs[order(-freqs)][1:20]

```
```{r echo=FALSE}
head(comunes,20)

```


Podemos ver que las palabras que han salido casi todas pertenecen a las cabeceras de los artículos, como era de esperar. Además de otras palabras comunes en textos legales, como delito, código...

Ahora hagámoslo con Udpipe:
Primero anotamos 

```{r}
annotations <- udpipe_annotate(udmodel_es, x = corpus)
annotations_df <- as.data.frame(annotations)

```

```{r echo=FALSE}
head(annotations_df[,5:10], 5)
```


Y usamos keywords_rake
```{r}
keywords <- keywords_rake(
  annotations_df,    
  term = "lemma",        #Usamos la columna "lemma" de la anotaciones
  group = "doc_id"       #Agrupaciones segun doc_id
)

```
```{r echo=FALSE}
head(keywords)
```


Parece que no esta detectando nada. Esto puede ocurrir por frecuencia de elementos como puntuación

```{r}
keywords <- keywords_rake(
  annotations_df,    
  term = "lemma",        #Usamos la columna "lemma" de la anotaciones
  group = "doc_id",       #Agrupaciones segun doc_id
  relevant = (annotations_df$xpos != "PUNCT")  #Nos permite seleccionar cuales filas son relevantes 
  )

keywords = keywords[order(-keywords$freq),]   #Ordenamos nuestro dataframe en 
#orden descendente según la frecuencia 

```

```{r echo=FALSE}
head(keywords,5)
cat("...")
tail(keywords,5)
```

Vemos que muchas letras sueltas y números que no nos interesan demasiado. Veamos como arreglar esto 

```{r}
table(annotations_df$xpos)
```

Algunos de estos no nos interesan: Por ejemplo:
```{r}
head(annotations_df[which(annotations_df$upos=="SYM"),5])
head(annotations_df[which(annotations_df$upos=="NUM"),5])
```

Entonces los quitaremos con el parametro relevant de keywords_rake (permite decir que filas no son relevantes). También podemos modificar ngram_max, que cambia la máxima cantidad de palabras que puede tener en cuenta como una entidad (por defecto es solo 2)

```{r}
keywords <- keywords_rake(
  annotations_df,    
  term = "lemma",        
  group = "doc_id",
  relevant = ( ! annotations_df$xpos  %in% c("PUNCT","SYM","NUM")),
  ngram_max = 5,
)

keywords = keywords[order(-keywords$freq),]  #Ordenamos for frecuencia

```

```{r echo=FALSE}
head(keywords,10)
cat("...")
tail(keywords,5)
```


Siguen saliendo muchas palabras que no nos interesan "a", "y", "el que". Para ello quitaremos otras categorías como las conjunciones y los determinantes. Así se concentrará más en los sustantivos y adjetivos

```{r}
keywords <- keywords_rake(
  annotations_df,    
  term = "lemma",        
  group = "doc_id",
  relevant = ( ! annotations_df$xpos  %in% c("PUNCT","SYM","NUM","CCONJ","ADP")),
  ngram_max = 5,
)

keywords = keywords[order(-keywords$freq),]  #Ordenamos for frecuencia

```

```{r echo =FALSE}
head(keywords,15)
cat("...")
tail(keywords,5)
```

Ahora parece representar algo mejor los textos.
Podemos ver que coincide en algunas con las calculadas con spacyr, "delitos", "año", "juez".


## Otros:

Se han creado varias funciones auxiliares para dividir en bloques la tarea de hacer scraping y formatear los datos, de esta manera es más fácil de buscar errores y organizar el código. El resto de los archivos son:

- **Schema.txt**: contiene un esquema de las cabeceras de los artículos. Esto fue obtenido de antemano usando XPATH

- **generador.R**:
    - get_html_arts: devuelve los links de los artículos que nos interesan
    - writeAll: escribe en la carpeta artículos el contenido (en html) de los artículos ( escoge automáticamente solo aquellos que falten)
    
- **processor.R**:
    - get_articles: devuelve los nombres de los artículos que nos interesan en el formato deseado (Articulo 10)
    - get_content: recibe el contenido html de un artículo y devuelve el contenido real del artículo del código penal
    - get_titles: itera sobre schema.txt y crea un dataframe que incluye cada artículo con sus cabeceras (Libro, Título, Capítulo, Sección)
    - separator: utiliza el dataframe dado por get_titles y lo separa en 3 elementos: 1. un dataframe `<<docvars>>` que contiene el mismo dataframe anterior pero quitando el inicio de las cabeceras, es decir, pasa de `<<LIBRO I: de las disposiciones...>>` a `<<de las disposiciones...>>`, 2. un vector `<<names>>`, que contiene el nombre de cada artículo con el fomato `<<LIBRO I.Título II.Capítulo I.Artículo III.>>` (Por ejemplo) y 3. un vector `<<starts>>` que contiene los inicios de los artículos, que es solamente la concatenación, con  `"\n"` como separador, de las cabeceras al completo de cada artículo
    
- **Grupo: 9**

- **Correos:**

  Pablo de Tarso Pedraz: tarso.pedraz@alumnos.upm.es
  Jorge López: jorge.lopez.diaz@alumnos.upm.es
  Martín Hernández: martin.hcastano@alumnos.upm.es
  Pablo Suárez: pablo.suarez@alumnos.upm.es
  
- **Obstáculos**

La parte más larga fue con diferencia la primera, la creación del corpus. Una de las complicaciones fue que no todos los artículos que aparecían debían ser scrapeados, por eso optamos por directamente leer un archivo de referencia (archivo_ref.txt) para guiarnos, y eliminiar aquellos suprimidos, frente a nuestra primera opción donde scrapeábamos directamente de la página web sin guía. Otra complicación fue añadir las cabeceras, pues algunos artículos tenían de una pero no de la otra (por ejemplo estar dentro de un capítulo pero no haber secciones), aunque se llegó a una solución óptima y "sencilla". Otros problemas fueron: la falta de formarto (algunas palabras llevaban tilde y otras no, algunas estaban en mayúscula y otros no...) y la existencia de valores en blanco que R no detectaba como espacios.

Además, durante la tercera parte nos hallamos estancados durante un buen tiempo tras intentar usar keywords_rake y tener un resultado vacío. Intentamos eliminiar la puntuación usando sub pero seguía sin funcionar. Leímos repetidamente la guía del comando keywords_rake pero no decía necesitar usar el parametro relevant (predeterminado todo esta a True), por ello debería ser lo mismo usar el corpus sin puntuaciónes que el keywords_rake con el relevant puesto a "todos menos puntuaciones", sin embargo por alguna razón no es así. Nos dimos cuenta de que había que añadirlo al intentar probar con los ejemplos que vienen en la guía del comando. 
  
- **Autorías**

Los ejercicios no fueron repartidos desde el principio sino que fueron realizados sobre la marcha por diferentes integrantes. Los encargados de cada parte, con sus relativas autorías fueron:

- Parte 1: Jorge López 0.4 - Pablo Suárez 0.6
- Parte 2: Pablo de Tarso 0.7 - Jorge López 0.2 - Martín Hernández 0.1
- Parte 3: Martín Hernández 0.7 - Pablo Suárez 0.3
